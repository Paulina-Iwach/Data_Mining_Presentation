{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "126bfc9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <div style=\"text-align: center; font-size: 100px;\">Introduction to ensemble clustering</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced4a60",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Idea and purpose of cluster analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f70983",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<figure>\n",
    "    <img src=\"k-means-clustering.png\" width=\"900\" height=\"600\" alt=\"K-means Clustering\">\n",
    "    <figcaption>Source: https://www.javatpoint.com/k-means-clustering-algorithm-in-machine-learning</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e24e0",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cluster analysis is an example of unsupervised machine learning and focuses on detecting the internal structure of the data, such that each cluster contains objects that are similar to each other.\n",
    "\n",
    "* Cluster - group of objects in a dataset that are similar to each other\n",
    "* Partition - division of the entire dataset into a finite number K of subsets (clusters), where each data point belongs to exactly one subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40476ca",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891d553",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An overview of the most important clustering algorithm types\n",
    "\n",
    "A common division of cluster analysis algorithms includes the following:\n",
    "- clustering methods,\n",
    "- hierarchical methods,\n",
    "- others: methods based on probabilistic distributions, methods using graph theory and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389d649",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"Cluster_types.png\"/></center>\n",
    "<div style=\"font-size: 16px;\">Source: Gao, C. X., Wang, S., Zhu, Y., Ziou, M., Teo, S. M., Smith, C. L., ... & Dwyer, D. (2024). Ensemble clustering: A practical tutorial.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851fb08f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Potential problems of clustering \n",
    "* Application of two different clustering algorithms to the same dataset may produce completely different results.\n",
    "* Evaluating the performance of clustering models is relatively challenging.\n",
    "* Basic clustering algorithms often have problems with detecting unusual cluster shapes.\n",
    "\n",
    "To tackle these problems, <span style=\"font-weight: bold;\"> ensemble clustering </span>might be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d35e6",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is ensemble clustering (a.k.a. consensus clustering)?\n",
    "Ensemble clustering combines the creation of multiple partitions for a given dataset and using the relationships observed between them to determine a final, single partition. The goal is to find a more reliable, improved solution than with individual grouping.\n",
    "Clustering ensemble is always made up of two steps:\n",
    "* Generate $M$ partitions $P = \\{P_1, P_2, \\dots, P_M\\} $ using selected clustering algorithms.\n",
    "* Combining the obtained partitions into one final partition $P^{*}$ using a consensus function $\\Gamma$.\n",
    "<br>\n",
    "<center><img src=\"clusters_graph.png\" style=\"width: 70%; height: 50%;\"/></center>\n",
    "<div style=\"font-size: 16px;\">Source: Vega-Pons, Sandro, and José Ruiz-Shulcloper. \"A survey of clustering ensemble algorithms.\" International Journal of Pattern Recognition and Artificial Intelligence 25.03 (2011): 337-372.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cab311",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties that an ensemble clustering algorithm should posses\n",
    "* <span style=\"font-weight: bold;\">Robustness</span> - the combination of results should have better average performance than the single clustering algorithms,\n",
    "* <span style=\"font-weight: bold;\">Consistency</span> - combination result should be close to all combined results of the individual clustering algorithms,\n",
    "* <span style=\"font-weight: bold;\">Novelty</span> - ensemble clustering should achieve results that are unattainable by standard clustering algorithms,\n",
    "* <span style=\"font-weight: bold;\">Stability</span> - the results obtained shall be less sensitive to noise and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79abccba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First step - Generation mechanism\n",
    "It involves creating multiple diverse partitions (base clusters), which will then be used to obtain consensus. The main objective is to generate a variety of clustering results by using different approaches and parameters. This helps  to highlight different structural aspects of the dataset and can lead to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9757cd1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key methods of generating partitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d636bd64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"BC.png\"/></center>\n",
    "<div style=\"font-size: 16px;\">Source: Vega-Pons, Sandro, and José Ruiz-Shulcloper. \"A survey of clustering ensemble algorithms.\" International Journal of Pattern Recognition and Artificial Intelligence 25.03 (2011): 337-372.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65132fd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different Objects Representations\n",
    "Data objects in a dataset can be represented in multiple ways by transforming or selecting different features. These representations can reveal different characteristics and patterns within the data. For example, in one pratition, features can be transformed using a logarithmic scale.\n",
    "* <span style=\"font-weight: bold;\">Advantage:</span> Transformed features can mitigate the impact of outliers, normalize distributions, and highlight different patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62a11a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different Clustering Algorithms\n",
    "\n",
    "Apply multiple clustering algorithms to the same dataset to obtain diverse partitions.\n",
    "* <span style=\"font-weight: bold;\">Advantage:</span> The unique properties and assumptions of each algorithm can lead to a broader exploration of clusters.\n",
    "<br>\n",
    "\n",
    "## Different Parameters Initialization\n",
    "\n",
    "Use the same clustering algorithm with different initial parameters to generate different partitions.\n",
    "* <span style=\"font-weight: bold;\">Advantage:</span> Increase the robustness and accuracy of the final clustering result by exploring sensitivity to initial conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8772d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projection to Subspaces\n",
    "Technique involves projecting the data into different subspaces. It works on the entire dataset and includes, for example, dimensionality reduction.\n",
    "* <span style=\"font-weight: bold;\">Advantage:</span> The method is particularly useful for multidimensional datasets, as it helps to capture different structural aspects of the data that may not be visible in the original feature space.\n",
    "<br>\n",
    "\n",
    "## Different Subsets of Objects\n",
    "\n",
    "Involves generating multiple partitions of the dataset using different subsets of the data (e.g. randomly selected). Works on subsets of the dataset, but keeping constant feature space.\n",
    "* <span style=\"font-weight: bold;\">Advantage:</span> Suitable for large data sets due to the reduction in computational load."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6831fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Second step - Consensus Functions\n",
    "Consensus functions are mathematical or algorithmic procedures that combine the results of clustering into one final solution (transforming the $M$ components of a partition $P$ into a final partition $P^*$).\n",
    "\n",
    "There are two main methods for obtaining consensus:\n",
    "* using information about which class labels have been given to observations in each partitions. This method is called  <span style=\"font-weight: bold;\">Median Partition</span> and focuses on finding the partition that maximises similarity to all partitions in the cluster ensemble.\n",
    "* using information on how often different objects were assigned to the same cluster (known as  <span style=\"font-weight: bold;\">Co-Occurrence</span>).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d42cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main categories of consensus functions\n",
    "* Direct Methods (Majority Voting) - use assigned cluster labels directly and apply different majority voting schemes.\n",
    "* Feature-Based Methods - \n",
    "* Method based on co-association matrix - \n",
    "* Graph methods \n",
    "* Information theory and mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7fd0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"consensus_types.png\"/></center>\n",
    "<div style=\"text-align: center; font-size: 16px;\">\n",
    "    Source: Gao, C. X., Wang, S., Zhu, Y., Ziou, M., Teo, S. M., Smith, C. L., ... & Dwyer, D. (2024). Ensemble clustering: A practical tutorial.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e6e98e",
   "metadata": {},
   "source": [
    "Zalety i wady ensamble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071402ec",
   "metadata": {},
   "source": [
    "K-means opisać "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de353b5d",
   "metadata": {},
   "source": [
    "Macierz co-ass + pierwsza metoda na niej \n",
    "Z macierzy do spectral ensamble "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827ac68",
   "metadata": {},
   "source": [
    "macierz -> graf ->treshold "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8769f09",
   "metadata": {},
   "source": [
    "Pierwsza metoda u Marcina batch majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3b11a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
