{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "126bfc9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <div style=\"text-align: center; font-size: 100px;\">Introduction to ensemble clustering</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced4a60",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " ## Idea and purpose of cluster analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f70983",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<figure>\n",
    "    <img src=\"pictures_theory/k-means-clustering.png\" width=\"900\" height=\"600\" alt=\"K-means Clustering\">\n",
    "    <figcaption>Source: https://www.javatpoint.com/k-means-clustering-algorithm-in-machine-learning</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667e24e0",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Cluster analysis is an example of unsupervised machine learning and focuses on detecting the internal structure of the data, such that each cluster contains objects that are similar to each other.\n",
    "\n",
    "* Cluster - group of objects in a dataset that are similar to each other\n",
    "* Partition - division of the entire dataset into a finite number K of subsets (clusters), where each data point belongs to exactly one subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40476ca",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891d553",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An overview of the most important clustering algorithm types\n",
    "\n",
    "A common division of cluster analysis algorithms includes the following:\n",
    "- clustering methods,\n",
    "- hierarchical methods,\n",
    "- others: methods based on probabilistic distributions, methods using graph theory and many others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389d649",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"pictures_theory/Cluster_types.png\"/></center>\n",
    "<div style=\"font-size: 16px;\">Source: Gao, C. X., Wang, S., Zhu, Y., Ziou, M., Teo, S. M., Smith, C. L., ... & Dwyer, D. (2024). Ensemble clustering: A practical tutorial.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458b62c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-means clustering algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463583cb",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "K-means method is one of the most popular \n",
    "cluster algorithms. The method is based on an iterative allocation of observations to the nearest centres, which are randomly selected at the beginning. The method uses Euclidean distance as a measure of dissimilarity and can be applied only to continuous data.\n",
    "\n",
    "The new centroids for each cluster are calculated from this mean formula\n",
    "$$\\begin{equation*}\n",
    "m_k = \\frac{1}{|C_k|} \\sum_{x_i \\in C_k} x_i\n",
    "\\end{equation*}$$\n",
    "where $m_k$ is new centroid of cluster $C_k$ and $x_i$ is a data point that belongs to $C_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0593f",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"pictures_theory/kmeans.png\"/></center>\n",
    "<div style=\"font-size: 16px;\">Source: Bustamam, A., et al. \"Application of k-means clustering algorithm in grouping the DNA sequences of hepatitis B virus (HBV).\" AIP Conference Proceedings. Vol. 1862. No. 1. AIP Publishing, 2017.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851fb08f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Potential problems of clustering \n",
    "* Application of two different clustering algorithms to the same dataset may produce completely different results.\n",
    "* Evaluating the performance of clustering models is relatively challenging.\n",
    "* Basic clustering algorithms often have problems with detecting unusual cluster shapes.\n",
    "\n",
    "To tackle these problems, <span style=\"font-weight: bold;\"> ensemble clustering </span>might be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d35e6",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is ensemble clustering (a.k.a. consensus clustering)?\n",
    "Ensemble clustering combines the creation of multiple partitions for a given dataset and using the relationships observed between them to determine a final, single partition. The goal is to find a more reliable, improved solution than with individual grouping.\n",
    "Clustering ensemble is always made up of two steps:\n",
    "* Generate $M$ partitions $P = \\{P_1, P_2, \\dots, P_M\\} $ using selected clustering algorithms.\n",
    "* Combining the obtained partitions into one final partition $P^{*}$ using a consensus function $\\Gamma$.\n",
    "<br>\n",
    "<center><img src=\"pictures_theory/clusters_graph.png\" style=\"width: 70%; height: 50%;\"/></center>\n",
    "<div style=\"font-size: 16px;\">Source: Vega-Pons, Sandro, and José Ruiz-Shulcloper. \"A survey of clustering ensemble algorithms.\" International Journal of Pattern Recognition and Artificial Intelligence 25.03 (2011): 337-372.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cab311",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties that an ensemble clustering algorithm should posses\n",
    "* <span style=\"font-weight: bold;\">Robustness</span> - the combination of results should have better average performance than the single clustering algorithms,\n",
    "* <span style=\"font-weight: bold;\">Consistency</span> - combination result should be close to all combined results of the individual clustering algorithms,\n",
    "* <span style=\"font-weight: bold;\">Novelty</span> - ensemble clustering should achieve results that are unattainable by standard clustering algorithms,\n",
    "* <span style=\"font-weight: bold;\">Stability</span> - the results obtained shall be less sensitive to noise and outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79abccba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First step - Generation mechanism\n",
    "It involves creating multiple diverse partitions (base clusters), which will then be used to obtain consensus. The main objective is to generate a variety of clustering results by using different approaches and parameters. This helps  to highlight different structural aspects of the dataset and can lead to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9757cd1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key methods of generating partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636bd64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"pictures_theory/BC.png\"/></center>\n",
    "<div style=\"font-size: 16px;\">Source: Vega-Pons, Sandro, and José Ruiz-Shulcloper. \"A survey of clustering ensemble algorithms.\" International Journal of Pattern Recognition and Artificial Intelligence 25.03 (2011): 337-372.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65132fd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different Objects Representations\n",
    "Data objects in a dataset can be represented in multiple ways by transforming or selecting different features. These representations can reveal different characteristics and patterns within the data. For example, in one pratition, features can be transformed using a logarithmic scale.\n",
    "* <span style=\"font-weight: bold;\">Advantage:</span> Transformed features can mitigate the impact of outliers, normalize distributions, and highlight different patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62a11a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Different Clustering Algorithms\n",
    "\n",
    "Apply multiple clustering algorithms to the same dataset to obtain diverse partitions.\n",
    "* <span style=\"font-weight: bold;\">Advantage:</span> The unique properties and assumptions of each algorithm can lead to a broader exploration of clusters.\n",
    "<br>\n",
    "\n",
    "## Different Parameters Initialization\n",
    "\n",
    "Use the same clustering algorithm with different initial parameters to generate different partitions.\n",
    "* <span style=\"font-weight: bold;\">Advantage:</span> Increase the robustness and accuracy of the final clustering result by exploring sensitivity to initial conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8772d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projection to Subspaces\n",
    "Technique involves projecting the data into different subspaces. It works on the entire dataset and includes, for example, dimensionality reduction.\n",
    "* <span style=\"font-weight: bold;\">Advantage:</span> The method is particularly useful for multidimensional datasets, as it helps to capture different structural aspects of the data that may not be visible in the original feature space.\n",
    "<br>\n",
    "\n",
    "## Different Subsets of Objects\n",
    "\n",
    "Involves generating multiple partitions of the dataset using different subsets of the data (e.g. randomly selected). Works on subsets of the dataset, but keeping constant feature space.\n",
    "* <span style=\"font-weight: bold;\">Advantage:</span> Suitable for large data sets due to the reduction in computational load."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6831fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Second step - Consensus Functions\n",
    "Consensus functions are mathematical or algorithmic procedures that combine the results of clustering into one final solution (transforming the $M$ components of a partition $P$ into a final partition $P^*$).\n",
    "\n",
    "There are two main methods for obtaining consensus:\n",
    "* using information about which class labels have been given to observations in each partitions. This method is called  <span style=\"font-weight: bold;\">Median Partition</span> and focuses on finding the partition that maximises similarity to all partitions in the cluster ensemble.\n",
    "* using information on how often different objects were assigned to the same cluster (known as  <span style=\"font-weight: bold;\">Co-Occurrence</span>).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3d42cb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Main categories of consensus functions\n",
    "* <span style=\"font-weight: bold;\">Direct Methods (Majority Voting)</span> - use assigned cluster labels directly and apply different majority voting schemes.\n",
    "* <span style=\"font-weight: bold;\">Feature-Based Methods</span> -  treat cluster labels as categorical (nominal) features and use algorithms that operate based on these features.\n",
    "* <span style=\"font-weight: bold;\">Method based on co-association matrix</span> - use similarity between pairs of observations based on the frequency of grouping pairs of data points in different partitions.\n",
    "* <span style=\"font-weight: bold;\">Graph methods</span> - formulate the clustering problem as an issue in graph theory.\n",
    "* <span style=\"font-weight: bold;\">Information theory and mixture model</span> - use information-theoretic measures and probabilistic models to define and optimize the consensus partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de6bf71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Co-association Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ad0177",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "A matrix used to measure the frequency with which pairs of data points appear in the same cluster across multiple partitions. \n",
    "For each partition $P_k$, where $k = 1,2, \\dots, M$ the co-association matrix $W_k$ is determined with elements\n",
    "$$w_{ij} = \\frac{1}{M} \\sum_{k=1}^{M} w_{ij}^{(k)},$$\n",
    "where\n",
    "$$w_{ij}^{(k)} = \\begin{cases}\n",
    "    1, & \\text{if observations } i \\text{ and } j \\text{ are in the same cluster in partition } k, \\\\\n",
    "    0, & \\text{otherwise}.\n",
    "    \\end{cases}$$\n",
    "    <br>\n",
    " <center><img src=\"pictures_theory/co_matrix.png\"  width=\"40%\" height=\"30%\"/></center>\n",
    "<div style=\"text-align: center; font-size: 16px;\">Source: Zhong, C., Luo, T., & Yue, X. (2018). Cluster ensemble based on iteratively refined co-association matrix. IEEE Access, 6, 69210-69223.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c7fd0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"pictures_theory/consensus_types.png\"/></center>\n",
    "<div style=\"text-align: center; font-size: 16px;\">\n",
    "    Source: Gao, C. X., Wang, S., Zhu, Y., Ziou, M., Teo, S. M., Smith, C. L., ... & Dwyer, D. (2024). Ensemble clustering: A practical tutorial.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f2a0fd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e6e98e",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Advantages\n",
    "* Improved performance and accuracy compared to single models.\n",
    "* Ensemble methods are generally more robust to noise and outliers.\n",
    "* Reduce the risk of overfitting and underfitting.\n",
    "* Flexibility, by using the strengths of different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071402ec",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Challenges\n",
    "* More complicated to implement and understand.\n",
    "* Computational expensive and time consuming due to the need to train and store multiple models.\n",
    "* Ensamble methods can be sensitive to the quality and diversity of the data and base models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de353b5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Macierz co-ass + pierwsza metoda na niej \n",
    "Z macierzy do spectral ensamble "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b827ac68",
   "metadata": {},
   "source": [
    "macierz -> graf ->treshold "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8769f09",
   "metadata": {},
   "source": [
    "Pierwsza metoda u Marcina batch majority"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484a1f0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sources:\n",
    "* [1] Wu, Xiuge, et al. \"A comparative study of clustering ensemble algorithms.\" Computers & Electrical Engineering 68 (2018): 603-615. (https://www.sciencedirect.com/science/article/abs/pii/S0045790617325417?via%3Dihub)\n",
    "* [2] Vega-Pons, Sandro, and José Ruiz-Shulcloper. \"A survey of clustering ensemble algorithms.\" International Journal of Pattern Recognition and Artificial Intelligence 25.03 (2011): 337-372. (https://www.researchgate.net/publication/220360297_A_Survey_of_Clustering_Ensemble_Algorithms)\n",
    "* [3] Gao, C. X., Wang, S., Zhu, Y., Ziou, M., Teo, S. M., Smith, C. L., ... & Dwyer, D. (2024). Ensemble clustering: A practical tutorial.\n",
    "* [4] Zhong, Caiming, Ting Luo, and Xiaodong Yue. \"Cluster ensemble based on iteratively refined co-association matrix.\" IEEE Access 6 (2018): 69210-69223. (https://www.semanticscholar.org/paper/Cluster-Ensemble-Based-on-Iteratively-Refined-Zhong-Luo/9c46b21f8d7d89d1e967388f96933bafde033675)\n",
    "* [5] https://www.linkedin.com/advice/1/what-pros-cons-using-ensemble-methods-ml-skills-algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25909fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
